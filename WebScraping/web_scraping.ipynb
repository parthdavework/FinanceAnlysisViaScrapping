{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07010101",
   "metadata": {},
   "source": [
    "Importing all the required packages for the web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdcefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from urllib.error import URLError\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342d12",
   "metadata": {},
   "source": [
    "Defining class as AI_Dev_project and defining methods under the class for each purpose\n",
    "\n",
    "\n",
    "News parse- It will web scrape all the descriptions of each news from page 1 to 30 ans save it in news.csv\n",
    "\n",
    "search stock - It will scrape all the toronto stock exchange symbols from the description of the dataframe\n",
    "\n",
    "storing stock symbols - It will scrape the symbols and create a list of unique values of all the TSX symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526f5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_Dev_project:\n",
    "    def __init__(self,webpage): # main instance\n",
    "        self.webpage=webpage  #storing the webpage as instance to be called later on from other methods\n",
    "    def news_parse(self):  # method1 for new parsing\n",
    "        data=[]\n",
    "        start_time_1=time.time()  # starting start time of the parsing\n",
    "        try:\n",
    "            response=requests.get(self.webpage)\n",
    "        except:\n",
    "            print(\"Error found\")  \n",
    "        else:\n",
    "            print(\"connection successfull\")\n",
    "            for i in range(1,51):  # parsing first 50 pages\n",
    "                link = self.webpage + str(i)  # appending page numbers from 1 to 30\n",
    "                soup = requests.get(link)  # scraping\n",
    "                logging.warning('Page {} scraping. Please wait !!!'.format(i))\n",
    "                soup = BeautifulSoup(soup.content, 'html.parser')\n",
    "                for j in soup.find_all('p',class_=\"remove-outline\"): # desciption is stored in p class in the html parsed soup\n",
    "                    data.append(j)\n",
    "                    \n",
    "            self.df=pd.DataFrame(data,columns=['News description'])   # storing in dataframe\n",
    "            self.df.to_csv('../data_storage/datanews.csv')            #writing a csv\n",
    "            print('Time taken:',time.time()-start_time_1) # time calculation after parsing\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9614e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=AI_Dev_project('https://www.prnewswire.com/news-releases/news-releases-list/?page=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beeb8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Page 1 scraping. Please wait !!!\n",
      "WARNING:root:Page 2 scraping. Please wait !!!\n",
      "WARNING:root:Page 3 scraping. Please wait !!!\n",
      "WARNING:root:Page 4 scraping. Please wait !!!\n",
      "WARNING:root:Page 5 scraping. Please wait !!!\n",
      "WARNING:root:Page 6 scraping. Please wait !!!\n",
      "WARNING:root:Page 7 scraping. Please wait !!!\n",
      "WARNING:root:Page 8 scraping. Please wait !!!\n",
      "WARNING:root:Page 9 scraping. Please wait !!!\n",
      "WARNING:root:Page 10 scraping. Please wait !!!\n",
      "WARNING:root:Page 11 scraping. Please wait !!!\n",
      "WARNING:root:Page 12 scraping. Please wait !!!\n",
      "WARNING:root:Page 13 scraping. Please wait !!!\n",
      "WARNING:root:Page 14 scraping. Please wait !!!\n",
      "WARNING:root:Page 15 scraping. Please wait !!!\n",
      "WARNING:root:Page 16 scraping. Please wait !!!\n",
      "WARNING:root:Page 17 scraping. Please wait !!!\n",
      "WARNING:root:Page 18 scraping. Please wait !!!\n",
      "WARNING:root:Page 19 scraping. Please wait !!!\n",
      "WARNING:root:Page 20 scraping. Please wait !!!\n",
      "WARNING:root:Page 21 scraping. Please wait !!!\n",
      "WARNING:root:Page 22 scraping. Please wait !!!\n",
      "WARNING:root:Page 23 scraping. Please wait !!!\n",
      "WARNING:root:Page 24 scraping. Please wait !!!\n",
      "WARNING:root:Page 25 scraping. Please wait !!!\n",
      "WARNING:root:Page 26 scraping. Please wait !!!\n",
      "WARNING:root:Page 27 scraping. Please wait !!!\n",
      "WARNING:root:Page 28 scraping. Please wait !!!\n",
      "WARNING:root:Page 29 scraping. Please wait !!!\n",
      "WARNING:root:Page 30 scraping. Please wait !!!\n",
      "WARNING:root:Page 31 scraping. Please wait !!!\n",
      "WARNING:root:Page 32 scraping. Please wait !!!\n",
      "WARNING:root:Page 33 scraping. Please wait !!!\n",
      "WARNING:root:Page 34 scraping. Please wait !!!\n",
      "WARNING:root:Page 35 scraping. Please wait !!!\n",
      "WARNING:root:Page 36 scraping. Please wait !!!\n",
      "WARNING:root:Page 37 scraping. Please wait !!!\n",
      "WARNING:root:Page 38 scraping. Please wait !!!\n",
      "WARNING:root:Page 39 scraping. Please wait !!!\n",
      "WARNING:root:Page 40 scraping. Please wait !!!\n",
      "WARNING:root:Page 41 scraping. Please wait !!!\n",
      "WARNING:root:Page 42 scraping. Please wait !!!\n",
      "WARNING:root:Page 43 scraping. Please wait !!!\n",
      "WARNING:root:Page 44 scraping. Please wait !!!\n",
      "WARNING:root:Page 45 scraping. Please wait !!!\n",
      "WARNING:root:Page 46 scraping. Please wait !!!\n",
      "WARNING:root:Page 47 scraping. Please wait !!!\n",
      "WARNING:root:Page 48 scraping. Please wait !!!\n",
      "WARNING:root:Page 49 scraping. Please wait !!!\n",
      "WARNING:root:Page 50 scraping. Please wait !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 60.590999126434326\n",
      "Before multithreading:                                        News description\n",
      "0     This holiday season, taste and serve the best ...\n",
      "1     The \"Flow Cytometry in Oncology and Immunology...\n",
      "2     \"I thought there could be an improved way to c...\n",
      "3     Nearly 350,000 Teamster retirees are celebrati...\n",
      "4     Martell, the oldest of the great Cognac houses...\n",
      "...                                                 ...\n",
      "1245  Frost & Sullivan recently researched the Latin...\n",
      "1246  Vicore Pharma Holding AB (publ) (\"Vicore\" or t...\n",
      "1247  Bit Digital, Inc. (Nasdaq: BTBT) (the \"Company...\n",
      "1248  Китайский народ скорбит о кончине бывшего лиде...\n",
      "1249  Populus Financial Group returned to the classr...\n",
      "\n",
      "[1250 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Before multithreading:',obj.news_parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42d7fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After multithreading\n",
      "connection successfull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Page 1 scraping. Please wait !!!\n",
      "WARNING:root:Page 2 scraping. Please wait !!!\n",
      "WARNING:root:Page 3 scraping. Please wait !!!\n",
      "WARNING:root:Page 4 scraping. Please wait !!!\n",
      "WARNING:root:Page 5 scraping. Please wait !!!\n",
      "WARNING:root:Page 6 scraping. Please wait !!!\n",
      "WARNING:root:Page 7 scraping. Please wait !!!\n",
      "WARNING:root:Page 8 scraping. Please wait !!!\n",
      "WARNING:root:Page 9 scraping. Please wait !!!\n",
      "WARNING:root:Page 10 scraping. Please wait !!!\n",
      "WARNING:root:Page 11 scraping. Please wait !!!\n",
      "WARNING:root:Page 12 scraping. Please wait !!!\n",
      "WARNING:root:Page 13 scraping. Please wait !!!\n",
      "WARNING:root:Page 14 scraping. Please wait !!!\n",
      "WARNING:root:Page 15 scraping. Please wait !!!\n",
      "WARNING:root:Page 16 scraping. Please wait !!!\n",
      "WARNING:root:Page 17 scraping. Please wait !!!\n",
      "WARNING:root:Page 18 scraping. Please wait !!!\n",
      "WARNING:root:Page 19 scraping. Please wait !!!\n",
      "WARNING:root:Page 20 scraping. Please wait !!!\n",
      "WARNING:root:Page 21 scraping. Please wait !!!\n",
      "WARNING:root:Page 22 scraping. Please wait !!!\n",
      "WARNING:root:Page 23 scraping. Please wait !!!\n",
      "WARNING:root:Page 24 scraping. Please wait !!!\n",
      "WARNING:root:Page 25 scraping. Please wait !!!\n",
      "WARNING:root:Page 26 scraping. Please wait !!!\n",
      "WARNING:root:Page 27 scraping. Please wait !!!\n",
      "WARNING:root:Page 28 scraping. Please wait !!!\n",
      "WARNING:root:Page 29 scraping. Please wait !!!\n",
      "WARNING:root:Page 30 scraping. Please wait !!!\n",
      "WARNING:root:Page 31 scraping. Please wait !!!\n",
      "WARNING:root:Page 32 scraping. Please wait !!!\n",
      "WARNING:root:Page 33 scraping. Please wait !!!\n",
      "WARNING:root:Page 34 scraping. Please wait !!!\n",
      "WARNING:root:Page 35 scraping. Please wait !!!\n",
      "WARNING:root:Page 36 scraping. Please wait !!!\n",
      "WARNING:root:Page 37 scraping. Please wait !!!\n",
      "WARNING:root:Page 38 scraping. Please wait !!!\n",
      "WARNING:root:Page 39 scraping. Please wait !!!\n",
      "WARNING:root:Page 40 scraping. Please wait !!!\n",
      "WARNING:root:Page 41 scraping. Please wait !!!\n",
      "WARNING:root:Page 42 scraping. Please wait !!!\n",
      "WARNING:root:Page 43 scraping. Please wait !!!\n",
      "WARNING:root:Page 44 scraping. Please wait !!!\n",
      "WARNING:root:Page 45 scraping. Please wait !!!\n",
      "WARNING:root:Page 46 scraping. Please wait !!!\n",
      "WARNING:root:Page 47 scraping. Please wait !!!\n",
      "WARNING:root:Page 48 scraping. Please wait !!!\n",
      "WARNING:root:Page 49 scraping. Please wait !!!\n",
      "WARNING:root:Page 50 scraping. Please wait !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 17.429576873779297\n"
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as p:\n",
    "    print('After multithreading')\n",
    "    p.map(obj.news_parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb087f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468275d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61590b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab5534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2d054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386adf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818f416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65973b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d53d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb0802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377a68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
