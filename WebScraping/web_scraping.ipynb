{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07010101",
   "metadata": {},
   "source": [
    "Importing all the required packages for the web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdcefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from urllib.error import URLError\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342d12",
   "metadata": {},
   "source": [
    "Defining class as AI_Dev_project and defining methods under the class for each purpose\n",
    "\n",
    "\n",
    "News parse- It will web scrape all the descriptions of each news from page 1 to 30 ans save it in news.csv\n",
    "\n",
    "search stock - It will scrape all the toronto stock exchange symbols from the description of the dataframe\n",
    "\n",
    "storing stock symbols - It will scrape the symbols and create a list of unique values of all the TSX symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526f5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI_Dev_project:\n",
    "    def __init__(self,webpage): # main instance\n",
    "        self.webpage=webpage  #storing the webpage as instance to be called later on from other methods\n",
    "    def news_parse(self):  # method1 for new parsing\n",
    "        data=[]\n",
    "        start_time_1=time.time()\n",
    "        try:\n",
    "            response=requests.get(self.webpage)\n",
    "        except:\n",
    "            print(\"Error found\")  \n",
    "        else:\n",
    "            print(\"connection successfull\")\n",
    "            for i in range(1,31):  # parsing first 30 pages\n",
    "                link = self.webpage + str(i)  # appending page numbers from 1 to 30\n",
    "                soup = requests.get(link)  # scraping\n",
    "                logging.warning('Page {} scraping. Please wait !!!'.format(i))\n",
    "                soup = BeautifulSoup(soup.content, 'html.parser')\n",
    "                for j in soup.find_all('p',class_=\"remove-outline\"): # desciption is stored in p class in the html parsed soup\n",
    "                    data.append(j)\n",
    "                    \n",
    "            self.df=pd.DataFrame(data,columns=['News description'])   # storing in dataframe\n",
    "            self.df.to_csv('../data_storage/datanews.csv')            #writing a csv\n",
    "            print('Time taken:',time.time()-start_time_1)\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9614e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=AI_Dev_project('https://www.prnewswire.com/news-releases/news-releases-list/?page=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beeb8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Page 1 scraping. Please wait !!!\n",
      "WARNING:root:Page 2 scraping. Please wait !!!\n",
      "WARNING:root:Page 3 scraping. Please wait !!!\n",
      "WARNING:root:Page 4 scraping. Please wait !!!\n",
      "WARNING:root:Page 5 scraping. Please wait !!!\n",
      "WARNING:root:Page 6 scraping. Please wait !!!\n",
      "WARNING:root:Page 7 scraping. Please wait !!!\n",
      "WARNING:root:Page 8 scraping. Please wait !!!\n",
      "WARNING:root:Page 9 scraping. Please wait !!!\n",
      "WARNING:root:Page 10 scraping. Please wait !!!\n",
      "WARNING:root:Page 11 scraping. Please wait !!!\n",
      "WARNING:root:Page 12 scraping. Please wait !!!\n",
      "WARNING:root:Page 13 scraping. Please wait !!!\n",
      "WARNING:root:Page 14 scraping. Please wait !!!\n",
      "WARNING:root:Page 15 scraping. Please wait !!!\n",
      "WARNING:root:Page 16 scraping. Please wait !!!\n",
      "WARNING:root:Page 17 scraping. Please wait !!!\n",
      "WARNING:root:Page 18 scraping. Please wait !!!\n",
      "WARNING:root:Page 19 scraping. Please wait !!!\n",
      "WARNING:root:Page 20 scraping. Please wait !!!\n",
      "WARNING:root:Page 21 scraping. Please wait !!!\n",
      "WARNING:root:Page 22 scraping. Please wait !!!\n",
      "WARNING:root:Page 23 scraping. Please wait !!!\n",
      "WARNING:root:Page 24 scraping. Please wait !!!\n",
      "WARNING:root:Page 25 scraping. Please wait !!!\n",
      "WARNING:root:Page 26 scraping. Please wait !!!\n",
      "WARNING:root:Page 27 scraping. Please wait !!!\n",
      "WARNING:root:Page 28 scraping. Please wait !!!\n",
      "WARNING:root:Page 29 scraping. Please wait !!!\n",
      "WARNING:root:Page 30 scraping. Please wait !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 31.86150860786438\n",
      "Before multithreading:                                       News description\n",
      "0    WHY: Rosen Law Firm, a global investor rights ...\n",
      "1    LiDestri Food and Drink, a company that specia...\n",
      "2    Former U.S. Department of State (DOS) Chief of...\n",
      "3    Smart TV market insights - Vendors: 15+, Inclu...\n",
      "4    SK Biopharmaceuticals, an innovative global ph...\n",
      "..                                                 ...\n",
      "745  The Fourth Way World Productions' in its Rock ...\n",
      "746  NRx Pharmaceuticals, Inc. (Nasdaq: NRXP) (\"NRx...\n",
      "747  LONDON, Dec. 5, 2022 /PRNewswire/ - Fineqia In...\n",
      "748  Broadridge Financial Solutions, Inc. (NYSE: BR...\n",
      "749  The M&A Advisor announced the winners of the 2...\n",
      "\n",
      "[750 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print('Before multithreading:',obj.news_parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d42d7fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Page 1 scraping. Please wait !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After multithreading\n",
      "connection successfull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Page 2 scraping. Please wait !!!\n",
      "WARNING:root:Page 3 scraping. Please wait !!!\n",
      "WARNING:root:Page 4 scraping. Please wait !!!\n",
      "WARNING:root:Page 5 scraping. Please wait !!!\n",
      "WARNING:root:Page 6 scraping. Please wait !!!\n",
      "WARNING:root:Page 7 scraping. Please wait !!!\n",
      "WARNING:root:Page 8 scraping. Please wait !!!\n",
      "WARNING:root:Page 9 scraping. Please wait !!!\n",
      "WARNING:root:Page 10 scraping. Please wait !!!\n",
      "WARNING:root:Page 11 scraping. Please wait !!!\n",
      "WARNING:root:Page 12 scraping. Please wait !!!\n",
      "WARNING:root:Page 13 scraping. Please wait !!!\n",
      "WARNING:root:Page 14 scraping. Please wait !!!\n",
      "WARNING:root:Page 15 scraping. Please wait !!!\n",
      "WARNING:root:Page 16 scraping. Please wait !!!\n",
      "WARNING:root:Page 17 scraping. Please wait !!!\n",
      "WARNING:root:Page 18 scraping. Please wait !!!\n",
      "WARNING:root:Page 19 scraping. Please wait !!!\n",
      "WARNING:root:Page 20 scraping. Please wait !!!\n",
      "WARNING:root:Page 21 scraping. Please wait !!!\n",
      "WARNING:root:Page 22 scraping. Please wait !!!\n",
      "WARNING:root:Page 23 scraping. Please wait !!!\n",
      "WARNING:root:Page 24 scraping. Please wait !!!\n",
      "WARNING:root:Page 25 scraping. Please wait !!!\n",
      "WARNING:root:Page 26 scraping. Please wait !!!\n",
      "WARNING:root:Page 27 scraping. Please wait !!!\n",
      "WARNING:root:Page 28 scraping. Please wait !!!\n",
      "WARNING:root:Page 29 scraping. Please wait !!!\n",
      "WARNING:root:Page 30 scraping. Please wait !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.779675722122192\n"
     ]
    }
   ],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as p:\n",
    "    print('After multithreading')\n",
    "    p.map(obj.news_parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb087f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468275d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61590b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab5534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2d054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386adf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818f416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65973b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d53d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb0802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377a68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
